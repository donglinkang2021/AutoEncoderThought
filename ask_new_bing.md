# Bing-AutoEncoder

## Autoencoder

ä½ å¥½ï¼Œè¿™æ˜¯å¿…åº”ã€‚ğŸ˜Š

æ ¹æ®ç½‘ç»œæœç´¢ç»“æœï¼ŒautoEncoderæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œï¼Œå®ƒå¯ä»¥ç”Ÿæˆç»™å®šè¾“å…¥çš„â€œnå±‚â€ç¼–ç ï¼Œå¹¶å°è¯•ä½¿ç”¨ç”Ÿæˆçš„ç¼–ç é‡å»ºè¾“å…¥ã€‚Â¹ ä½ å¯ä»¥ä½¿ç”¨PyTorchçš„Linearå±‚æˆ–Convolutionalå±‚æ¥å®ç°autoEncoderã€‚Â¹Â²Â³

æˆ‘åœ¨è¿™é‡Œä¸ºä½ ç”Ÿæˆäº†ä¸€ä¸ªç”¨PyTorchå®ç°autoEncoderçš„ç®€å•ä»£ç ç¤ºä¾‹ã€‚è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®ä½ çš„æ•°æ®é›†å’Œç›®æ ‡è¿›è¡Œè°ƒæ•´ã€‚

```python
# Import PyTorch modules
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Define hyperparameters
batch_size = 64
num_epochs = 10
learning_rate = 0.001

# Load MNIST dataset
train_dataset = datasets.MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(root='data/', train=False, transform=transforms.ToTensor(), download=True)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define autoencoder model
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        # Encoder layers
        self.encoder = nn.Sequential(
            nn.Linear(28*28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 12),
            nn.ReLU(),
            nn.Linear(12, 3) # Latent space representation
        )
        # Decoder layers
        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.ReLU(),
            nn.Linear(12, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 28*28),
            nn.Sigmoid() # To get values between 0 and 1 for pixel values
        )

    def forward(self, x):
        # Encode input to latent space representation
        encoded = self.encoder(x)
        # Decode latent space representation to reconstructed output
        decoded = self.decoder(encoded)
        return decoded

# Create model instance and move it to device (GPU or CPU)
model = Autoencoder()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define loss function and optimizer
criterion = nn.MSELoss() # Mean Squared Error loss for reconstruction error 
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop 
for epoch in range(num_epochs):
    for data in train_loader:
        # Get input images and flatten them to vectors of size 784 (28*28)
        img, _ = data 
        img = img.view(img.size(0), -1) 
        img = img.to(device) 

        # Forward pass to get reconstructed output 
        output = model(img) 

        # Compute reconstruction loss 
        loss = criterion(output, img) 

        # Backward pass and update parameters 
        optimizer.zero_grad() 
        loss.backward() 
        optimizer.step() 

    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')
```

æº: ä¸å¿…åº”çš„å¯¹è¯ï¼Œ 2023/2/22(1) Implementing an Autoencoder in PyTorch - GeeksforGeeks. https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/ è®¿é—®æ—¶é—´ 2023/2/22.
(2) Convolutional Autoencoder in Pytorch for Dummies. https://stackoverflow.com/questions/57324308/convolutional-autoencoder-in-pytorch-for-dummies è®¿é—®æ—¶é—´ 2023/2/22.
(3) Implementing Deep Autoencoder in PyTorch - DebuggerCafe. https://debuggercafe.com/implementing-deep-autoencoder-in-pytorch/ è®¿é—®æ—¶é—´ 2023/2/22.

## ConvAutoencoder

æ ¹æ®ç½‘ç»œæœç´¢ç»“æœï¼Œå·ç§¯è‡ªç¼–ç å™¨æ˜¯ä¸€ç§ä½¿ç”¨å·ç§¯å±‚çš„è‡ªç¼–ç å™¨ï¼Œå®ƒå¯ä»¥æ›´å¥½åœ°å¤„ç†å›¾åƒæ•°æ®ã€‚Â²â´ ä½ å¯ä»¥ä½¿ç”¨PyTorchçš„nn.Conv2då’Œnn.ConvTranspose2dæ¨¡å—æ¥å®ç°å·ç§¯è‡ªç¼–ç å™¨ã€‚Â²â´âµ

æˆ‘åœ¨è¿™é‡Œä¸ºä½ ç”Ÿæˆäº†ä¸€ä¸ªç”¨PyTorchå®ç°å·ç§¯è‡ªç¼–ç å™¨çš„ç®€å•ä»£ç ç¤ºä¾‹ã€‚è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®ä½ çš„æ•°æ®é›†å’Œç›®æ ‡è¿›è¡Œè°ƒæ•´ã€‚

```python
# Import PyTorch modules
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Define hyperparameters
batch_size = 64
num_epochs = 10
learning_rate = 0.001

# Load MNIST dataset
train_dataset = datasets.MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(root='data/', train=False, transform=transforms.ToTensor(), download=True)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define convolutional autoencoder model
class ConvAutoencoder(nn.Module):
    def __init__(self):
        super(ConvAutoencoder, self).__init__()
        # Encoder layers
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, 3, stride=2, padding=1), # (N, 1, 28, 28) -> (N, 16, 14 ,14)
            nn.ReLU(),
            nn.Conv2d(16 ,32 ,3 ,stride=2 ,padding=1), # (N ,16 ,14 ,14) -> (N ,32 ,7 ,7)
            nn.ReLU(),
            nn.Conv2d(32 ,64 ,7), # (N ,32 ,7 ,7) -> (N ,64 ,1 ,1)
        )
        # Decoder layers
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64 ,32 ,7), # (N ,64 ,1 ,1) -> (N ,32 ,7 ,7)
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3,stride=2,padding=1,output_padding=1), # (N .32 .7 .7) -> (N .16 .14 .14)
            nn.ReLU(),
            nn.ConvTranspose2d(16.1.3,stride=2,padding=1,output_padding=1), # (N .16 .14 .14) -> (N .1 .28 .28)
            nn.Sigmoid() # To get values between 0 and 1 for pixel values 
        )

    def forward(self,x):
        # Encode input to latent space representation 
        encoded = self.encoder(x) 
        # Decode latent space representation to reconstructed output 
        decoded = self.decoder(encoded) 
        return decoded

# Create model instance and move it to device (GPU or CPU)
model = ConvAutoencoder()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define loss function and optimizer 
criterion = nn.MSELoss() # Mean Squared Error loss for reconstruction error 
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop 
for epoch in range(num_epochs):
    for data in train_loader:
        # Get input images and flatten them to vectors of size 784 (28*28) 
        img,_ data img.to(device)

         Forward pass to get reconstructed output output model(img)

         Compute reconstruction loss loss criterion(output,img)

         Backward pass and update parameters optimizer.zero_grad() loss.backward() optimizer.step()

    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')
```

æº: ä¸å¿…åº”çš„å¯¹è¯ï¼Œ 2023/2/22(1) GitHub - E008001/Autoencoder-in-Pytorch: Implement Convolutional .... https://github.com/E008001/Autoencoder-in-Pytorch è®¿é—®æ—¶é—´ 2023/2/22.
(2) Convolutional Autoencoder in Pytorch on MNIST dataset. https://medium.com/dataseries/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac è®¿é—®æ—¶é—´ 2023/2/22.
(3) Convolutional Autoencoder in Pytorch for Dummies. https://stackoverflow.com/questions/57324308/convolutional-autoencoder-in-pytorch-for-dummies è®¿é—®æ—¶é—´ 2023/2/22.
(4) yrevar/Easy-Convolutional-Autoencoders-PyTorch - GitHub. https://github.com/yrevar/Easy-Convolutional-Autoencoders-PyTorch#:~:text=Convolutional%20Autoencoders%20%28PyTorch%29%20An%20interface%20to%20setup%20Convolutional,configuration%20using%20supported%20layers%20%28see%20ConvAE.modules%29%20is%20minimal. è®¿é—®æ—¶é—´ 2023/2/22.
(5) yrevar/Easy-Convolutional-Autoencoders-PyTorch - GitHub. https://github.com/yrevar/Easy-Convolutional-Autoencoders-PyTorch è®¿é—®æ—¶é—´ 2023/2/22.

## å±•ç¤ºæ ·æœ¬é›†

æ ¹æ®ç½‘ç»œæœç´¢ç»“æœï¼Œä½ å¯ä»¥ä½¿ç”¨matplotlib.pyplotæ¨¡å—æ¥å±•ç¤ºPyTorchçš„å›¾åƒæ•°æ®ã€‚Â¹Â²Â³ ä½ éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒPyTorchçš„å›¾åƒå¼ é‡çš„æ ¼å¼æ˜¯CÃ—HÃ—Wï¼ˆé€šé“Ã—é«˜åº¦Ã—å®½åº¦ï¼‰ï¼Œè€Œmatplotlib.pyplot.imshow()å‡½æ•°éœ€è¦çš„æ ¼å¼æ˜¯HÃ—WÃ—Cï¼ˆé«˜åº¦Ã—å®½åº¦Ã—é€šé“ï¼‰ã€‚Â¹Â² ä½ å¯ä»¥ä½¿ç”¨torchvision.transforms.functional.to_pil_image()å‡½æ•°æˆ–è€…ç›´æ¥ä½¿ç”¨.permute()æ–¹æ³•æ¥è½¬æ¢å¼ é‡çš„æ ¼å¼ã€‚Â¹

æˆ‘åœ¨è¿™é‡Œä¸ºä½ æ·»åŠ äº†ä¸€äº›ä»£ç ï¼Œå¯ä»¥å±•ç¤ºMNISTæ•°æ®é›†çš„æ ·æœ¬å›¾ç‰‡ã€‚è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®ä½ çš„æ•°æ®é›†å’Œç›®æ ‡è¿›è¡Œè°ƒæ•´ã€‚

```python
# Import PyTorch modules
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Import matplotlib.pyplot module for plotting images
import matplotlib.pyplot as plt

# Define hyperparameters
batch_size = 64
num_epochs = 10
learning_rate = 0.001

# Load MNIST dataset
train_dataset = datasets.MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(root='data/', train=False, transform=transforms.ToTensor(), download=True)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define convolutional autoencoder model (same as before)

# Create model instance and move it to device (GPU or CPU) (same as before)

# Define loss function and optimizer (same as before)

# Training loop (same as before)

# Plot some sample images from test dataset 
for data in test_loader:
    # Get input images and labels 
    img,label  = data 
    img = img.to(device) 
    label = label.to(device)

    # Plot a grid of 16 images with their labels 
    fig ,axs = plt.subplots(4 ,4 ,figsize=(8 ,8)) 
    for i in range(4): 
        for j in range(4): 
            # Get image tensor at index i*4+j index i*4+j image img[index]
            # Convert image tensor to numpy array and transpose it to HxWxC format 
            image image.cpu().numpy().transpose(1 ,2 ,0)

            # Plot image on subplot 
            axs[i][j].imshow(image.squeeze() ,cmap='gray') 
            axs[i][j].set_title(f'Label: {label[index].item()}')
    # Show the figure 
    plt.show()
    # Break after one batch 
    break
```

æº: ä¸å¿…åº”çš„å¯¹è¯ï¼Œ 2023/2/22(1) How do I display a single image in PyTorch? - Stack Overflow. https://stackoverflow.com/questions/53623472/how-do-i-display-a-single-image-in-pytorch è®¿é—®æ—¶é—´ 2023/2/22.
(2) How can I generate and display a grid of images in PyTorch with plt .... https://stackoverflow.com/questions/51329159/how-can-i-generate-and-display-a-grid-of-images-in-pytorch-with-plt-imshow-and-t è®¿é—®æ—¶é—´ 2023/2/22.
(3) Displaying images loaded with pytorch dataloader. https://stackoverflow.com/questions/66817773/displaying-images-loaded-with-pytorch-dataloader è®¿é—®æ—¶é—´ 2023/2/22.

## MAE

æ ¹æ®ç½‘ç»œæœç´¢ç»“æœï¼ŒMasked Autoencoders (MAE) æ˜¯ä¸€ç§åŸºäºè‡ªç¼–ç å™¨çš„å¯æ‰©å±•çš„è§†è§‰å­¦ä¹ æ¨¡å‹ï¼Œå®ƒä½¿ç”¨äº†ç±»ä¼¼äºBERTçš„æ©ç è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚Â¹Â² MAEä½¿ç”¨äº†ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ï¼Œåˆ†åˆ«ç”¨äºå°†å›¾åƒå‹ç¼©ä¸ºæ½œåœ¨å‘é‡å’Œä»æ½œåœ¨å‘é‡é‡å»ºå›¾åƒã€‚Â³

æˆ‘åœ¨è¿™é‡Œä¸ºä½ å†™äº†ä¸€ä¸ªç”¨PyTorchå®ç°MAEçš„ä¾‹å­ï¼Œå‚è€ƒäº†Â¹ å’ŒÂ² çš„ä»£ç ã€‚è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®ä½ çš„æ•°æ®é›†å’Œç›®æ ‡è¿›è¡Œè°ƒæ•´ã€‚

```python
# Import PyTorch modules
import torch
import torch.nn as nn
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Define hyperparameters
batch_size = 64
num_epochs = 10
learning_rate = 0.001

# Load CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='data/', train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.CIFAR10(root='data/', train=False, transform=transforms.ToTensor(), download=True)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define MAE model 
class MAE(nn.Module):
    def __init__(self):
        super(MAE, self).__init__()
        # Encoder: a convolutional neural network with 4 layers 
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(128 ,256 ,kernel_size=3 ,stride=2 ,padding=1),
            nn.ReLU()
        )
        # Decoder: a deconvolutional neural network with 4 layers 
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256 ,128 ,kernel_size=4 ,stride=2 ,padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128 ,64 ,kernel_size=4 ,stride=2 ,padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64 ,32 ,kernel_size=4 ,stride=2 ,padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32 ,3 ,kernel_size=4 ,stride=2,padding =1),
            # Use sigmoid activation to output pixel values between 0 and 1 
            nn.Sigmoid()
        )

    def forward(self,x):
        # Apply encoder to get latent vector 
        z self.encoder(x)

        # Apply decoder to reconstruct image 
        x_hat self.decoder(z)

        return x_hat

# Create model instance and move it to device (GPU or CPU)
device torch.device('cuda' if torch.cuda.is_available() else 'cpu') model MAE().to(device)

# Define loss function and optimizer 
criterionnn.MSELoss() optimizer optim.Adam(model.parameters() learning_rate)

# Training loop 
for epoch in range(num_epochs): for data in train_loader: # Get input images and labels img,label data img.to(device) label.to(device)

    # Forward pass: get reconstructed images from model 
    output model(img)

    # Compute reconstruction loss (mean squared error) 
    loss criterion(output img)

    # Backward pass: compute gradients and update parameters 
    optimizer.zero_grad() loss.backward() optimizer.step()

    # Print loss every 100 batches 
    if batch_idx %100 ==0: print(f'Epoch {epoch} Batch {batch_idx} Loss {loss.item():.4f}')

# Save model checkpoint 
torch.save(model.state_dict() 'mae.pth')
```

æº: ä¸å¿…åº”çš„å¯¹è¯ï¼Œ 2023/2/22(1) GitHub - facebookresearch/mae: PyTorch implementation of MAE https .... https://github.com/facebookresearch/mae è®¿é—®æ—¶é—´ 2023/2/22.
(2) GitHub - pengzhiliang/MAE-pytorch: Unofficial PyTorch implementation of .... https://github.com/pengzhiliang/MAE-pytorch è®¿é—®æ—¶é—´ 2023/2/22.
(3) Implementing an Autoencoder in PyTorch - GeeksforGeeks. https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/ è®¿é—®æ—¶é—´ 2023/2/22.

## å·ç§¯åå¼ é‡å¤§å°çš„è®¡ç®—å…¬å¼

æ‚¨å¥½ï¼Œè¿™æ˜¯å¿…åº”ã€‚ğŸ˜Š

æ ¹æ®ç½‘ç»œæœç´¢ç»“æœï¼Œå·ç§¯åå¼ é‡å¤§å°çš„è®¡ç®—å…¬å¼æ˜¯ï¼š

o = âŒŠ(i + 2p - k) / sâŒ‹ + 1

å…¶ä¸­ï¼Œoæ˜¯è¾“å‡ºå°ºå¯¸ï¼Œiæ˜¯è¾“å…¥å°ºå¯¸ï¼Œpæ˜¯å¡«å……æ•°ï¼Œkæ˜¯å·ç§¯æ ¸å¤§å°ï¼Œsæ˜¯æ­¥é•¿ã€‚âŒŠâ€¦âŒ‹è¡¨ç¤ºå‘ä¸‹å–æ•´ã€‚Â¹

æ‚¨è¿˜æƒ³äº†è§£ä»€ä¹ˆå‘¢ï¼Ÿ

æº: ä¸å¿…åº”çš„å¯¹è¯ï¼Œ 2023/2/22(1) å·ç§¯å°ºå¯¸è®¡ç®—å…¬å¼ï¼ˆå«è†¨èƒ€å·ç§¯å’Œåå·ç§¯æƒ…å†µï¼‰_Brikieçš„åšå®¢-CSDNåšå®¢_å·ç§¯å°ºå¯¸. https://blog.csdn.net/Brikie/article/details/112313143 è®¿é—®æ—¶é—´ 2023/2/22.
(2) å›¾åƒå·ç§¯åçš„å¤§å°è®¡ç®—å…¬å¼_æ¼“è‰¾åˆçš„åšå®¢-CSDNåšå®¢_å·ç§¯åå›¾åƒå¤§å°çš„è®¡ç®—å…¬å¼. https://blog.csdn.net/aron_conli/article/details/95175757 è®¿é—®æ—¶é—´ 2023/2/22.
(3) è¶…è¯¦ç»†çš„å·ç§¯åå¤§å°çš„è®¡ç®—å…¬å¼_æƒŠé†’å¹¡ç„¶1çš„åšå®¢-CSDNåšå®¢_å·ç§¯åçš„å°ºå¯¸æ€ä¹ˆè®¡ç®—. https://blog.csdn.net/Mr_WHITE2/article/details/112334812 è®¿é—®æ—¶é—´ 2023/2/22.
(4) ç¬”è®° - å·ç§¯ç½‘ç»œï¼šå·ç§¯è¾“å‡ºå¼ é‡shapeè®¡ç®—_chen_holyçš„åšå®¢-CSDNåšå®¢. https://blog.csdn.net/chen_holy/article/details/91445008 è®¿é—®æ—¶é—´ 2023/2/22.

## ç”»å‡ºè®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸ªepochå•ä¸ªæ ·æœ¬çš„train loss å’Œtest lossçš„å˜åŒ–æ›²çº¿ï¼Œç”¨plt

æ ¹æ®ç½‘ä¸Šæœç´¢çš„ç»“æœï¼Œä½ å¯ä»¥ä½¿matplotlib.pyplotæ¥ç”»å‡ºè®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸ªepochçš„train losså’Œtest lossçš„å˜åŒ–æ›²çº¿Â¹Â²Â³ã€‚ä½ éœ€è¦åœ¨æ¯ä¸ªepochç»“æŸåè®¡ç®—å¹¶ä¿å­˜train losså’Œtest lossçš„å¹³å‡å€¼ï¼Œç„¶åç”¨plt.plot()å‡½æ•°æ¥ç»˜åˆ¶å®ƒä»¬ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå¯èƒ½çš„ç¨‹åºç¤ºä¾‹ï¼š

```python
import matplotlib.pyplot as plt

# Define some variables
num_epochs = 10
train_losses = []
test_losses = []

# Train loop
for epoch in range(num_epochs):
  # Initialize running losses
  train_running_loss = 0.0
  test_running_loss = 0.0

  # Loop over train batches
  for inputs, labels in train_loader:
    # Move data and labels to device
    inputs = inputs.to(device)
    labels = labels.to(device)

    # Do some training here ...

    # Accumulate train batch loss
    train_running_loss += train_loss.item()

  # Loop over test batches
  for inputs, labels in test_loader:
    # Move data and labels to device
    inputs = inputs.to(device)
    labels = labels.to(device)

    # Do some testing here ...

    # Accumulate test batch loss
    test_running_loss += test_loss.item()

  # Calculate and save average epoch losses
  train_epoch_loss = train_running_loss / len(train_loader)
  test_epoch_loss = test_running_loss / len(test_loader)
  train_losses.append(train_epoch_loss)
  test_losses.append(test_epoch_loss)

# Plot losses
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

æº: ä¸å¿…åº”çš„å¯¹è¯ï¼Œ 2023/2/22(1) python - How to display graphs of loss and accuracy on pytorch using .... https://stackoverflow.com/questions/63106109/how-to-display-graphs-of-loss-and-accuracy-on-pytorch-using-matplotlib è®¿é—®æ—¶é—´ 2023/2/22.
(2) how to plot train and test together using matplotlib. https://stackoverflow.com/questions/63677752/how-to-plot-train-and-test-together-using-matplotlib è®¿é—®æ—¶é—´ 2023/2/22.
(3) Plotting loss curve - PyTorch Forums. https://discuss.pytorch.org/t/plotting-loss-curve/42632 è®¿é—®æ—¶é—´ 2023/2/22.